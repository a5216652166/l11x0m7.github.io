--- 
layout: post 
title: Tensorflow学习笔记（一） 基础
date: 2016-10-07 
categories: blog 
tags: [tensorflow, DeepLearning, NLP] 
description: Tensorflow
--- 

# Tensorflow学习笔记（一） 基础

## DeepLearning编程框架

* 用于参数配置：

caffe提供了很便捷的神经网络搭建和命令行工具，加之model zoo里面大量预训练好的模型(主要是图像相关的)可以做fine-tuning，因此使用在图像相关的研究和应用上非常方便。   

* 用于编程实现：

Theano以及搭建于其之上的Keras和Lasagne似乎颇受research派系同学的偏爱，自动求导是它的优势之一。   
MXnet对显存的利用率高，并且支持C++, Python, Julia, Matlab, JavaScript, Go, R, Scala这么多种语言，编写起来也比较简易。   
Torch是facebook用的深度学习package，定义新网络层比较简单，不过Lua倒不算大家熟知的编程语言。   
Tensorflow是Google提供资金研发的，比较全，支持分布式，

## 1.Tensorflow的名词解释

Tensorflow是Google开源的用于深度学习的框架。它的主要优点有两个：

* 所有的输入、参数输出都以张量的形式流动和传动；
* 一旦计算图（Computation Graph）绘制好，在运行图时就能够自动求导（使用BP的思路，就是链式法则反向传导误差），也就是说，你只需要写好损失函数，之后交给Tensorflow来处理就好了。

在Tensorflow里：  

* 使用张量(tensor)表示数据。  
* 使用图(graph)来表示计算任务。  
* 在被称之为会话(Session)的上下文(context)中执行图。  
* 通过变量 (Variable)维护状态。  
* 使用feed和fetch可以为任意的操作(arbitrary operation)赋值或者从其中获取数据。  

## 2.什么是张量（tensor）

张量可以看作是多重向量空间映射到实数域空间。说白了就是多维数组。

* 标量是张量（实数值映射到实数值）
* 向量是张量
* 矩阵是张量
* 矩阵的矩阵是张量


## 3.Tensorflow和numpy的区别

相同点：都提供n维数组  
不同点：numpy里有ndarray，而Tensorflow里有tensor；numpy不提供创建张量函数和求导，也不提供GPU支持。

## 4.Tensorflow和numpy的操作对比

#### numpy定义与操作  

```python
In [23]: import numpy as np
In [24]: a = np.zeros((2,2)); b = np.ones((2,2))
In [25]: np.sum(b, axis=1)
Out[25]: array([ 2.,  2.])
In [26]: a.shape
Out[26]: (2, 2)
In [27]: np.reshape(a, (1,4))
Out[27]: array([[ 0.,  0.,  0.,  0.]])
```

#### 对应的Tensorflow定义与操作

```python
In [31]: import tensorflow as tf
In [32]: tf.InteractiveSession()
In [33]: a = tf.zeros((2,2)); b = tf.ones((2,2))
In [34]: tf.reduce_sum(b, reduction_indices=1).eval()
Out[34]: array([ 2.,  2.], dtype=float32)
In [35]: a.get_shape()
Out[35]: TensorShape([Dimension(2), Dimension(2)]) # TensorShape类似于tuple
In [36]: sess = tf.InteractiveSession() # 建立一个会话
In [37]: tf.reshape(a, (1, 4)).eval() # eval函数是用于输出对象的具体值的函数，一般要加入Session才能跑
Out[36]: array([[ 0.,  0.,  0.,  0.]], dtype=float32)
```

为了方便记忆，我们把numpy和Tensorflow中的部分定义和操作做成了一张一一对应的表格，方便大家查看。  

![](http://odjt9j2ec.bkt.clouddn.com/tf1.png)

Tensorflow的输出要稍微注意一下，我们需要显式地输出(evaluation，也就是说借助`eval()`函数)！

## 5.Tensorflow的计算图

用Tensorflow编写的程序一般由两部分构成，一是构造部分，包含了计算流图，二是执行部分，通过session 来执行图中的计算。
我们先来看看怎么构建图。构件图的第一步是创建源节点(source op)。源节点不需要任何输入，它的输出传递给其它节点(op)做运算。python库中，节点构造器的返回值即当前节点的输出，这些返回值可以传递给其它节点(op)作为输入。
TensorFlow Python库中有一个默认图(default graph)，在默认图的基础上，节点构造器(op 构造器)可以为其增加节点。这个默认图对许多程序来说已经足够用了。

```python
import tensorflow as tf
# 创建一个常量节点， 产生一个1x2矩阵，这个op被作为一个节点
# 加到默认视图中
# 构造器的返回值代表该常量节点的返回值
matrix1 = tr.constant([[3., 3.]])

# 创建另一个常量节点, 产生一个2x1的矩阵
matrix2 = tr.constant([[2.], [2.]])

# 创建一个矩阵乘法matmul节点，把matrix1和matrix2作为输入：
product = tf.matmul(matrix1, matrix2)
```

上述代码生成的计算图如下：

![](https://www.processon.com/chart_image/57f895c4e4b009c4af49e5ed.png)

两个矩阵常量节点输入到product里，进行乘积计算。

>注：此时只是画好了图，并没有真正开始运行，需要把图放到会话（Session）里，程序才开始真正运行。


## 6.Session对象

上述的计算图只是一种形式，如果想要让它运行，需要启动一个`Session`（会话环境）。

>Session是用于张量计算的封装环境。具体参考[Session文档](https://www.tensorflow.org/versions/r0.8/api_docs/python/client.html#Session)。


## 7.Session与多GPU运算

Tensorflow是支持分布式的深度学习框架/包，这是因为它能将图定义转换成分布式执行的操作，以充分利用可以利用的计算资源（如CPU或GPU）。不过一般情况下，你不需要显式指定使用CPU还是GPU，Tensorflow能自动检测。如果检测到GPU，Tensorflow会优先使用找到的第一个GPU来执行操作。  
如果机器上有超过一个可用的GPU，默认状况下除了第一个外的其他GPU是不参与计算的。为了让Tensorflow使用这些GPU，你必须将节点运算明确地指派给它们执行。  

## 8.Tensorflow的变量张量Variable（常量张量为Constant）

`Variable`一般是可以被更新或更改的数值，即在流图运行过程中可以被不断动态调整的值。

## 9.Fetch（获取）操作

`sess.run([node1,node2,...])`一般表示我整个流图跑一次，然后显示`node1`、`node2`……的输出。

## 10.Feed（传入）操作

通常可以看作是将节点设置为抽象的（没有具体的数值，只是一个容器），之后在`run`里面才使用`feed_dict`传入具体的值。通常要搭配`placeholder`来作为容器使用。

```python
input1 = tf.placeholder(tf.types.float32)
input2 = tf.placeholder(tf.types.float32)
output = tf.mul(input1, input2)
```

#### 手动提供feed数据作为run的参数

```python
with tf.Session() as see:
  print sess.run([output], feed_dict={input:[7.]， input2:[2.]})
# print
# 结果是[array([ 14.], dtype=float32)]
```



## 参考

* [深度学习与自然语言处理(6)_斯坦福cs224d 一起来学Tensorflow part1](http://blog.csdn.net/han_xiaoyang/article/details/51871068)
* [CS224d: Deep Learning for Natural Language Processing](http://cs224d.stanford.edu/syllabus.html)