--- 
layout: post 
title: Clustering杂谈
date: 2016-10-21 
categories: blog 
tags: [聚类, 机器学习] 
description: 聚类算法
--- 

# Clustering杂谈

聚类算法有很多，主要的目标就是将一堆数据中相似的数据归到一起，而不去管这个数据的具体标签。归类的方法直接从数据点中获取，在机器学习上这种方法属于非监督学习（Unsupervised Learning）。  
常用的聚类算法，比较经典的有k-means、k-medoids、GMM等。比较现代和流行的是Spectral Clustering（谱聚类）和Hierarchical Clustering（层级聚类）。

## 1.k-means

k均值聚类的思路比较清晰，就是给每个数据分配一个具体的标签。使用k均值聚类需要事先确定聚类数。  
k-means的算法步骤如下：

1. 选定$K$个中心$\mu_k$的初值。这个过程通常是针对具体的问题有一些启发式的选取方法，或者大多数情况下采用随机选取的办法。因为前面说过k-means并不能保证全局最优，而是否能收敛到全局最优解其实和初值的选取有很大的关系，所以有时候我们会多次选取初值跑k-means，并取其中最好的一次结果。
2. 将每个数据点归类到离它最近的那个中心点所代表的cluster中。
用公式$\mu_k = \frac{1}{N_k}\sum_{j\in\text{cluster}_k}x_j $计算出每个cluster的新的中心点。
3. 重复第二步，一直到迭代了最大的步数或者前后的总误差$J$的值相差小于一个阈值为止。其中总误差$J$的公式如下：
	* $\displaystyle J = \sum_{n=1}^N\sum_{k=1}^K r_{nk} \|x_n-\mu_k\|^2$


k-means的迭代过程其实是简化的EM算法思想。

### 二分k-means
 
关于k-means的变种，还有二分k-means。二分k-means的算法步骤如下：


1. 将所有数据点看成一个簇（初始化的簇位置为所有点的均值）。
2. 对每一个簇，计算该簇的总误差，并对该簇进行k-均值聚类（k=2），计算将该簇一分为二后的总误差和。误差可以是簇中所有点到簇的聚类点的平均距离
3. 选择使得误差最小的那个簇进行划分操作。
4. 循环2到3，知道簇的数目为k为止。

## 2.k-medoids

k均值的簇坐标为所有簇点的均值（means），而k-medoids则是选取簇中存在的点，该点是距离同个簇的所有其他点距离和最小的点，即寻找中位数（medium）。  
k-medoids对于聚类点的选取要求没有k-means高，因此k-means能做的k-medoids也能做。k-medoids主要针对的是非数字化特征的数据，每个样本点之间只有一个差异度值。那么我们就可以构造一个距离矩阵（N*K），相应的误差公式如下：

$$\displaystyle\tilde{J} = \sum_{n=1}^N\sum_{k=1}^K r_{nk}\mathcal{V}(x_n,\mu_k)$$

$\mathcal{V}(x_n,\mu_k)$表示数据点到中心点的距离。而$r_{nk}$表示该数据点n是否在中心点k的簇里，是为1，否则为0。

可以看到k-means只要求得簇点均值即可，复杂度为O(N)。而k-medoids则要计算簇中每个点到其他点的距离，复杂度为O(N^2)。但是由于k-medoids找的是已有的点，它不会像k-means一样产生离群（outlier）的聚类点，也不会选择数据中的离群点为聚类点，因此k-medoids比k-means要更具健壮（robust）。

## 3.Gaussian Mixture Model（GMM）

k-means是给每个数据点分配到具体的簇，而GMM的思路则是更贝叶斯。它不给出每个数据点对应的簇，而是给出每个数据点由各个聚类点产生的概率，并且不同聚类点产生某个数据点的概率和为1。  
GMM的目的是，假定k个簇中每个簇产生的数据点满足n维高斯分布，估计产生目前观测的数据点的概率最大的k个簇的参数。即我们对下面的公式求最大值：

$$\displaystyle
\begin{aligned}
p(x) & = \sum_{k=1}^K p(k)p(x|k) \\
& = \sum_{k=1}^K \pi_k \mathcal{N}(x|\mu_k, \Sigma_k)
\end{aligned}
$$

考虑到各个数据点的分布独立，则有：

$$p(x)=\prod_{i=1}^N p(x_i)=\displaystyle
\sum_{i=1}^N \sum_{k=1}^K \pi_k \mathcal{N}(x_i|\mu_k, \Sigma_k)
$$

考虑单个数据点的概率比较小，许多很小的数字相乘起来在计算机里很容易造成浮点数下溢，因此我们使用对数处理上式：

$$\log p(x)=\displaystyle
\sum_{i=1}^N \log \left\{\sum_{k=1}^K \pi_k \mathcal{N}(x_i|\mu_k, \Sigma_k)\right\}
$$

下面通过EM算法迭代求解参数$\mu_k$、$\pi_k$和$\Sigma_k$:

1. E步
	* 对于每个数据$x_i$来说，它由第k个聚类点生成的概率为：
	$$\displaystyle
	\gamma(i, k) = \frac{\pi_k \mathcal{N}(x_i|\mu_k, \Sigma_k)}{\sum_{j=1}^K \pi_j\mathcal{N}(x_i|\mu_j, \Sigma_j)}$$
	* 在计算$\gamma(i, k)$的时候我们假定$\mu_k$和$\Sigma_k$均已知，我们将取上一次迭代所得的值（或者初始值）
2. M步
	* $$\displaystyle
\begin{aligned}
\mu_k & = \frac{1}{N_k}\sum_{i=1}^N\gamma(i, k)x_i \\
\Sigma_k & = \frac{1}{N_k}\sum_{i=1}^N\gamma(i,
k)(x_i-\mu_k)(x_i-\mu_k)^T \\
\pi_k & = N_k/N
\end{aligned}$$

	* 其中$N_k = \sum_{i=1}^N \gamma(i, k)$，N为总数据点数。

重复迭代E、M两步，直到对数似然函数的值收敛为止。一般到M步停止收敛。

### 实现

## 参考

* [漫谈 Clustering 系列](http://blog.pluskid.org/?page_id=78)