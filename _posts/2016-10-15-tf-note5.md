--- 
layout: post 
title: Tensorflow学习笔记（五） RNN语言模型（RNNLM-Model）
date: 2016-10-15 
categories: blog 
tags: [tensorflow, DeepLearning, NLP] 
description: rnnlm
--- 
# Previous

1.[Tensorflow学习笔记（一） 基础](http://skyhigh233.com/blog/2016/10/10/tf-note1/)  
2.[Tensorflow学习笔记（二） Toy Demo](http://skyhigh233.com/blog/2016/10/14/tf-note2/)  
3.[Tensorflow学习笔记（三） 使用Skip-Gram和CBOW训练Word Embedding](http://skyhigh233.com/blog/2016/10/14/tf-note3/)  
4.[Tensorflow学习笔记（四） 命名实体识别模型（NER-Model）](http://skyhigh233.com/blog/2016/10/14/tf-note4/)

# Tensorflow学习笔记（五） RNN语言模型（RNNLM-Model）

本节的内容主要来自于CS224d课程里的Pset2程序作业部分。希望通过这个部分，学习到以下内容：

* RNN的实现过程
* 明白这个模型的实现不是truncated（截断）的，它会一直BP求导到第一个时间步骤
* 困惑度和对数交叉熵损失的关系（对数交叉熵=log困惑度）
* 多维矩阵的转置、reshape和split要弄清楚
* 变量的共用，通过scope.reuse_variables()实现
* 序列损失sequence_loss可以看源码了解它的输入格式

## 需要的矩阵和python的知识

### 矩阵的转置和shape

```python
# 例1
a=np.array([[[2,3],[5,6]], [[2,7],[9,8]]])
print np.concatenate(a, 1)
print np.concatenate(a, 1).reshape(-1, 2)

output:
[[2 3 2 7]
 [5 6 9 8]]

[[2 3]
 [2 7]
 [5 6]
 [9 8]]

# 例2
print a.transpose([1,0,2])
print a.reshape(-1, 2)

output:
[
 [[2 3]
  [2 7]]
 
 [[5 6]
  [9 8]]
]
  
[[2 3]
 [5 6]
 [2 7]
 [9 8]]
```

### *args 和**kwargs的用法

python的打包和拆包。打包即把多个参数合成一个列表或字典，拆包则是把一个列表或字典拆解成多个参数。

* *在作为函数形参时对列表打包，\*\*在作为函数形参时对字典打包。
* *在作为函数实参时对列表拆包，\*\*在作为函数实参时对字典拆包。

```python
# 例1
def aa(*args, **kwargs):
    print args
    print kwargs

aa(1, 2, happy=3, unhappy=4)

output:
(1, 2)
{'unhappy': 4, 'happy': 3}




# 例2
def bb(a, b, happy, unhappy):
    print a, b
    print happy, unhappy

input1 = [1, 2]
input2 = {happy:3, unhappy:4}
aa(*input1, **input2)

output:
1 2
3 4
```

下面重新自己写一遍，包含注释。如果需要源码，可到最后的参考目录中的官网去下载。

## 代码

```python

```

##参考

1.[CS224d:Pset2](http://cs224d.stanford.edu/syllabus.html)